\section{手法と実験設定}\label{sec:method}
本章では，評価対象モデル，データセットと前処理，学習プロトコル，評価指標について説明する。実験環境は単一 GPU (NVIDIA RTX 3090) と 64GB RAM を想定し，すべての実験を同一ハードウェアで実施した。

\subsection{評価対象モデル}
ResNet-18 は 11.7M パラメータを持つ残差ネットワークであり，Batch Normalization により勾配消失を抑制する。\cite{he2016deep} Vision Transformer 系モデルとしては DeiT-Ti を採用し，5.7M パラメータで自己注意機構を実現する。\cite{touvron2021training} いずれのモデルもパラメータ初期化には He 初期化を使用し，最終分類層のみ学習率を 10 倍にスケーリングした。

\subsection{データセットと前処理}
CIFAR-10 は 32$\times$32 ピクセルのカラー画像 60,000 枚で構成され，10 クラスを含む。全実験でトレーニング／検証比率は 45,000:5,000 とし，検証セットはクラス分布が均等になるよう分割した。前処理は以下の通りである。
\begin{itemize}
  \item 標準化: 各チャネルに対しデータセット全体の平均と標準偏差で正規化。
  \item データ拡張: RandAugment (N=2, M=9) を適用し，Cutout (size=8) を併用。\cite{cubuk2020randaugment}
  \item Test-Time Augmentation: 水平反転と 10-crop 評価を実施。
\end{itemize}

\subsection{学習プロトコル}
CNN では SGD + Momentum (0.9) を用い，初期学習率 0.1 を Cosine Annealing によりエポック 90 で 0.0001 まで減衰させた。ViT では AdamW を採用し，学習率 5e-4 と Weight Decay 0.05 を設定，5 エポックの線形ウォームアップ後に Cosine Annealing を適用した。バッチサイズは両モデルとも 128，トレーニングは 90 エポックで早期終了は行わない。Label Smoothing ($\varepsilon=0.1$) を Transformer のみで有効化した。\cite{muller2019does}

\subsection{評価指標とログ}
主指標は検証セットの Top-1 精度であり，補助指標として Top-5 精度，クロスエントロピー損失，推論時間（CPU/GPU）を記録した。各設定で異なる 5 個の乱数シードを用いて実験を実行し，平均と標準偏差を報告する。学習曲線と TTA の影響を可視化する図表の作成に向けて，エポック毎のログを統一フォーマットで保存した。

% 図表候補: 学習率スケジュールおよびデータ拡張のサマリーフローチャート
